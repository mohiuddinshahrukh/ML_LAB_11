{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ijXSmi570_HD"
      },
      "source": [
        "# Bayesian Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L03BD6f00_HF"
      },
      "source": [
        "This lab introduces you to a Bayesian/probabilistic perspective of Machine Learning. In order to have full information about the underlying distributions, we will be using *synthetic datasets*, i.e. datasets that we sample from distributions that we define explicitly ourselves.\n",
        "\n",
        "These data sets will be used to perform a Bayesian linear regression. We are looking at the MAP estimate and the posterior distribution of the model parameters. In order to keep it simple, we restrict ourselves to one-dimensional input spaces."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hzW4bWuK0_HG"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZRUxCzVJ0_HH"
      },
      "source": [
        "## 1. Synthetic dataset for linear regression\n",
        "\n",
        "We model the conditional distribution $p(y \\mid x)$ given the marginal distribution $p(x)$ on $\\mathbb{R}$ and a function $f: \\mathbb{R} \\rightarrow \\mathbb{R}$  as follows:\n",
        "\n",
        "$$ p(y \\mid x) = f(x) + \\epsilon \\qquad \\left(\\epsilon \\sim \\mathcal{N}(0, \\sigma^2)\\right), $$\n",
        "\n",
        "where $\\epsilon \\sim \\mathcal{N}(0, \\sigma^2)$ means that $\\epsilon$ is a random variable normally distributed with zero mean and a variance of $\\sigma^2$. Note that $f$ actually only has to be defined for $x$ with nonzero probability (this set of $x$ is called the *support of $p$*). Thus, we have fully specified the joint distribution $p(x, y) = p(x) p(y \\mid x)$.\n",
        "\n",
        "The function $f$ controls the relationship between $x$ and $y$. For example, if we use a linear function for $f$, there will be a linear relationship between $x$ and $y$ (plus some noise); hence, in this case, a linear regression model will be adequate to predict $y$ from $x$. \n",
        "\n",
        "To generate a data set $D = \\{(x_i, y_i) \\in X \\times Y \\mid 1 \\leq i \\leq N\\}$ we sample tuples $(x, y)$ by first picking an $x$ from the distribution $p(x)$. To compute the $y$, we sample a noise-term $\\epsilon$ from the normal distribution with the specified mean and variance, and compute $y = f(x) + \\epsilon$. The output $(x, y)$ is then one sample of our data set. Since we sample independently and identically distributed (*i.i.d*) we do this process $N$ times to obtain $D$.\n",
        "\n",
        "As ground work we first learn about the <a href=\"http://docs.scipy.org/doc/scipy/reference/stats.html\">scipy.stats</a> package, which contains distributions to sample from or computing quantities such as the *probability density function* (*pdf*), and about Python generators."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "61iRmSbj0_HI"
      },
      "source": [
        "First import the uniform and normal distributions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NuWqP4go0_HJ"
      },
      "outputs": [],
      "source": [
        "from scipy.stats import uniform\n",
        "from scipy.stats import norm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j-AnAAwv0_HJ"
      },
      "outputs": [],
      "source": [
        "# Models uniform distribution on the interval [loc, loc+scale]\n",
        "b = uniform(loc=0, scale=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YZX17pqd0_HJ"
      },
      "outputs": [],
      "source": [
        "print (\"10 samples: {}\".format(b.rvs(10)))\n",
        "print (\"Probablity density at x = 0: {}\".format(b.pdf(0)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kOP-FUrQ0_HK"
      },
      "source": [
        "In the following code snippet we are sampling from a standard normal distribution and look at the distribution of the samples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y4UEJapY0_HL"
      },
      "outputs": [],
      "source": [
        "a = norm(loc=0, scale=1)\n",
        "\n",
        "# this creates a grid of 2x2 plots\n",
        "# axes is then a 2x2 np.ndarray\n",
        "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
        "\n",
        "# sample 1000 points from the 0.01 percentile to 0.99 percentile\n",
        "x = np.linspace(norm.ppf(0.001), norm.ppf(0.999), 1000) \n",
        "\n",
        "num_samples = [10, 100, 1000, 10000]\n",
        "# we go through the axes one by one, need to make the 2x2 matrix linear before\n",
        "for i, ax in enumerate(axes.reshape(4)):\n",
        "    normal_dist = norm(loc=0, scale=1)\n",
        "    ax.plot(x, normal_dist.pdf(x), 'g', lw=3, alpha=.8)\n",
        "    \n",
        "    ys = normal_dist.rvs(size=num_samples[i])\n",
        "    ax.hist(ys, density=True, bins=100, histtype=\"stepfilled\", alpha=0.3)\n",
        "    ax.set_title('standard normal pdf with hist of {} samples'.format(num_samples[i]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JlwdcEBF0_HM"
      },
      "source": [
        "Generators are a special kind of functions in Python. Instead of returning only a single value, they behave like an iterator, i.e.,  return a (possibly infinite) sequence of values. Syntactically, the only difference between a 'normal' function and a generator is that  a generator uses the <b>yield</b> keyword rather than <b>return</b>. Once <b>yield</b> is used in the definition of a function,  the <b>next()</b> method is generated automatically. Each time, the generator's <b>next()</b> method is called, the generator function is executed until the <b>yield</b> keyword is reached. Similar to <b>return</b>, it returns the value given as parameter. If the generator is told to generate more values (using <b>next()</b>), the execution continues from the point of the last call of <b>yield</b>. Typically, <b>yield</b> is inside a (usually infinite) loop.  As an example, we write a generator that generates the natural numbers $\\mathbb{N}_0$, i.e. $0, 1, 2, \\dots$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ELF-zld60_HM"
      },
      "outputs": [],
      "source": [
        "def natural_numbers():\n",
        "    i = 0\n",
        "    \n",
        "    while True: # infinite loop!\n",
        "        yield i\n",
        "        i += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "viPxZEnj0_HM"
      },
      "outputs": [],
      "source": [
        "gen = natural_numbers()\n",
        "print (type(gen))\n",
        "print (next(gen))\n",
        "print ([next(gen) for i in range(100)]) # take 100 numbers, note that 0 was already generated!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ijeLS2cc0_HN"
      },
      "source": [
        "<b>Exercise 1</b>: Complete the code of the following three generators:\n",
        "\n",
        "- uniform_gen(a,b): generates a randomly sampled element from a uniform distribution with boundaries a and b. Assert that a < b.\n",
        "\n",
        "- normal_gen(mean, std): generates a randomly sampled element from a normal distribution with mean=mean and variance=stdÂ². Assert that std >= 0.\n",
        "\n",
        "- data_gen(f, x_gen, noise_gen): Generates data points (x, y), where x is assumed to be sampled using a generator function x_gen and y is f(x) + eps, where eps is assumed to be sampled using a generator function noise_gen (see introduction above). \n",
        "\n",
        "    You don't need to implement the two generator functions for x and eps here. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UlBqVZpN0_HN"
      },
      "outputs": [],
      "source": [
        "def uniform_generator(a, b):\n",
        "    ##############################\n",
        "    #### INSERT YOUR CODE HERE ###\n",
        "    ##############################\n",
        "    \n",
        "    #This is just the natural number generator\n",
        "    i = 0\n",
        "    \n",
        "    while True: # infinite loop!\n",
        "        yield i\n",
        "        i += 1\n",
        "        \n",
        "def normal_generator(mean, std):\n",
        "    ##############################\n",
        "    #### INSERT YOUR CODE HERE ###\n",
        "    ##############################\n",
        "    \n",
        "    #This is just the natural number generator\n",
        "    i = 0\n",
        "    \n",
        "    while True: # infinite loop!\n",
        "        yield i\n",
        "        i += 1\n",
        "    \n",
        "        \n",
        "def data_generator(f, x_gen, noise_gen):\n",
        "    ##############################\n",
        "    #### INSERT YOUR CODE HERE ###\n",
        "    ##############################\n",
        "    \n",
        "    #This is just the natural number generator (adapted to fit the data generator format)\n",
        "    i = 0\n",
        "    \n",
        "    while True: # infinite loop!\n",
        "        yield i,0\n",
        "        i += 1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xelGj5Se0_HO"
      },
      "outputs": [],
      "source": [
        "import itertools\n",
        "from scipy.stats import normaltest\n",
        "    \n",
        "try: \n",
        "    num_gen = uniform_generator(1, 5)\n",
        "    numbers = [next(num_gen) for _ in range(1000)]\n",
        "    \n",
        "    assert all(1 <= num and num <= 5 for num in numbers), \"The uniform generator\"\n",
        "    \n",
        "    normal_gen = normal_generator(0, 1)\n",
        "    numbers_normal = [next(normal_gen) for _ in range(1000)]\n",
        "    \n",
        "    k2, p = normaltest(numbers_normal)\n",
        "    \n",
        "    assert p > 1e-3, \"The normal generator\"\n",
        "   \n",
        "    def constant_generator(c):\n",
        "        while True:\n",
        "            yield c\n",
        "\n",
        "    data_gen = data_generator(lambda x: x**2, uniform_generator(-1, 1), constant_generator(0))\n",
        "    \n",
        "    assert all([x**2 == y for x, y in itertools.islice(data_gen, 0, 1000)]), \"The data generator\"\n",
        "    \n",
        "    print (\"The code seems fine.\")\n",
        "    \n",
        "except AssertionError as err:\n",
        "    print (\"Currently there is a fault in: \" + str(err))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l83s35jv0_HO"
      },
      "source": [
        "## 2. Posterior Distribution of $\\theta$ and MAP\n",
        "\n",
        "We now perform a Bayesian linear regression on data that is generated as explained in the introduction above. x is sampled from a uniform distribution with lower bound 0 and upper bound 5. The label y of an instance x is generated via $f(x) = 5x + 3$ plus some normally distributed  noise eps that has a standard deviation of $2.5$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BANNsmBr0_HP"
      },
      "outputs": [],
      "source": [
        "sigma   = 2.5 # sigma of the noise, do not change this!\n",
        "\n",
        "data_gen = data_generator(lambda x: 5*x + 3, uniform_generator(0, 5), normal_generator(0, sigma))\n",
        "data = list(itertools.islice(data_gen, 0, 100)) # list of pairs (x, y)\n",
        "x, y = zip(*data) # The asterisk unpacks data; i.e., this line corresponds to x,y=zip((x[0], y[0]), ((x[1], y[1])), ...)                         "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3arOEtcS0_HP"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(8, 8))\n",
        "plt.scatter(x, y)\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('y')\n",
        "plt.title(\"Sampled data set\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V7B7CV0p0_HP"
      },
      "source": [
        "We also add a bias. More precisely we extend the vector $x$ to a matrix by writing it as a column and adding a vector of $1$ next to it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dK5ALCQj0_HQ"
      },
      "outputs": [],
      "source": [
        "N = len(x)\n",
        "X = np.column_stack((np.asarray(x), np.ones(N)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NNjPoYPt0_HQ"
      },
      "source": [
        "<b>Exercise 2</b>: Let $X$ be a data matrix with dimensions $N \\times D$,  $y$ a label vector of dimension $D$. Moreover, let $\\sigma$ be a noise parameter  (standard deviation of the output noise) and $\\sigma_p$ the standard deviation for the prior distribution of the weights $\\theta$. Implement the missing parts of the following functions:\n",
        "\n",
        "- get_MAP: Returns the MAP estimate $\\theta_{\\text{MAP}}$ given $X, y, \\sigma, \\sigma_p$.\n",
        "\n",
        "$$ \\theta_{\\text{MAP}} = \\left(X^T X + \\frac{\\sigma^2}{\\sigma_p^2} I\\right)^{-1} X^T y. $$\n",
        "\n",
        "- get_posterior_distribution_parameters: Returns the mean vector (equal to the MAP estimate) and the covariance matrix $\\Sigma$ for the posterior distribution of $\\theta$ given $X$ and $y$.\n",
        "\n",
        "$$ \\Sigma = \\frac{1}{\\sigma^2} X^T X + \\frac{1}{\\sigma_p^2} I. $$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6QccGgFj0_HQ"
      },
      "outputs": [],
      "source": [
        "def get_MAP(X, y, sigma, sigma_p):\n",
        "    ##############################\n",
        "    #### INSERT YOUR CODE HERE ###\n",
        "    ##############################\n",
        "    return None #theta_MAP\n",
        "\n",
        "def get_posterior_distribution_parameters(X, y, sigma, sigma_p):\n",
        "    ##############################\n",
        "    #### INSERT YOUR CODE HERE ###\n",
        "    ##############################\n",
        "    return None # theta_MAP, covariance_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dfi6Cmo90_HQ"
      },
      "outputs": [],
      "source": [
        "sigma_p = 5\n",
        "\n",
        "theta_MAP = get_MAP(X, y, sigma=sigma, sigma_p=sigma_p)\n",
        "print(\"theta (MAP estimate): {}\".format(theta_MAP))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s2CvRmbn0_HQ"
      },
      "source": [
        "To use multivariate normal distributions, a different distribution has to be imported from scipy.stats. We visualize the posterior distribution of the weights."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "ZqZTanX-0_HR"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "from matplotlib.cbook import mplDeprecation\n",
        "from scipy.stats import multivariate_normal\n",
        "\n",
        "theta_MAP, Sigma = get_posterior_distribution_parameters(X, y, sigma, sigma_p)\n",
        "\n",
        "mvn = multivariate_normal(theta_MAP,np.linalg.inv(Sigma))\n",
        "thetas = mvn.rvs(5)\n",
        "\n",
        "def generate_contour(mvn, ax):\n",
        "    a, b = np.mgrid[4:6:.01, 1.5:4:.01]\n",
        "    pos = np.dstack((a, b))\n",
        "    ax = fig.add_subplot(111)\n",
        "    ax.contourf(a, b, mvn.pdf(pos))\n",
        "    \n",
        "fig = plt.figure(figsize=(8, 8))\n",
        "ax = fig.add_subplot(111)\n",
        "ax.set_title(\"Posterior of theta\")\n",
        "ax.set_xlabel(\"slope\")\n",
        "ax.set_ylabel(\"bias\")\n",
        "generate_contour(mvn, ax)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hOZrUlmd0_HR"
      },
      "source": [
        "We visualize the MAP estimate and sample posterior curves."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OsfRv-7i0_HR"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure(figsize=(8, 8))\n",
        "plt.scatter(x, y)\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('y')\n",
        "plt.title(\"MAP-estimate and posterior curves\")\n",
        "\n",
        "ps = np.linspace(0, 5, 1000)\n",
        "\n",
        "def predict(theta):\n",
        "    return lambda x: theta[0] * x + theta[1]\n",
        "\n",
        "plt.plot(ps, list(map(predict(theta_MAP), ps)), \"r-\", label=\"MAP\")\n",
        "\n",
        "for theta in thetas:\n",
        "    plt.plot(ps, list(map(predict(theta), ps)), \"g-\", alpha=0.6)\n",
        "plt.legend([\"MAP\", \"samples from posterior\"], loc=\"best\")\n",
        "plt.xlim([0, 5])\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.10"
    },
    "colab": {
      "name": "Bayesian_Learning.ipynb",
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}